<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-12-07T19:36:11+00:00</updated><id>/feed.xml</id><title type="html">James Hope</title><subtitle></subtitle><author><name>James Hope</name></author><entry><title type="html">Beyond declarative flows in virtual assistants with language models for single-Turn and multi-turn Reasoning</title><link href="/generativeai/conversationalai/beyond-declarative-flows-in-virtual-assistants/" rel="alternate" type="text/html" title="Beyond declarative flows in virtual assistants with language models for single-Turn and multi-turn Reasoning" /><published>2023-12-06T00:00:00+00:00</published><updated>2023-12-06T00:00:00+00:00</updated><id>/generativeai/conversationalai/beyond-declarative-flows-in-virtual-assistants</id><content type="html" xml:base="/generativeai/conversationalai/beyond-declarative-flows-in-virtual-assistants/">&lt;p&gt;Building user journeys as declarative trees within a virtual assistant requires assumptions to be made about the user query and the optimal path. This is excessbated if there are many decision points and tree consists of many forks as assumptions about the path increase exponentially as the journey progresses further down the tree. To address this inefficiency of design, one approach is to use a language model to reason over available tools (or APIs) that might be used to respond to the query. This approach collapses decision points in the tree and replaces it with a language model. Additionally, the language model can be guided through a policy or rules expressed in natural language and supplied to the model in a prompt.&lt;/p&gt;

&lt;p&gt;The following diagram shows this interaction with IBM Watson Assistant which is used to orchestrate the call to the language model for reasoning, the tools, and the language model again to generate a final response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/single-turn-reasoning.png&quot; alt=&quot;Interaction Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the language model is used for single turn reasoning. With next generation, stateful language models, multi-turn reasoning may be more effective at guiding the user to a goal.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="generativeAI" /><category term="conversationalAI" /><category term="llms" /><category term="generativeAI" /><category term="architecture" /><category term="watsonx" /><summary type="html">Building user journeys as declarative trees within a virtual assistant requires assumptions to be made about the user query and the optimal path. This is excessbated if there are many decision points and tree consists of many forks as assumptions about the path increase exponentially as the journey progresses further down the tree. To address this inefficiency of design, one approach is to use a language model to reason over available tools (or APIs) that might be used to respond to the query. This approach collapses decision points in the tree and replaces it with a language model. Additionally, the language model can be guided through a policy or rules expressed in natural language and supplied to the model in a prompt.</summary></entry><entry><title type="html">Supervised fine tuning of a large language model using quantized low rank adapters</title><link href="/generativeai/conversationalai/lora-fine-tuning/" rel="alternate" type="text/html" title="Supervised fine tuning of a large language model using quantized low rank adapters" /><published>2023-12-01T00:00:00+00:00</published><updated>2023-12-01T00:00:00+00:00</updated><id>/generativeai/conversationalai/lora-fine-tuning</id><content type="html" xml:base="/generativeai/conversationalai/lora-fine-tuning/">&lt;p&gt;Fine-tuning of a large language model (LLM) can be peformed using QLoRA (Quantized Low Rank Adapters) and PEFT (Parameter-Efficient Fine-Tuning) techniques.&lt;/p&gt;

&lt;p&gt;PEFT (Parameter-Efficient Fine-Tuning):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PEFT is a technique for fine-tuning large language models with a small number of additional parameters, known as adapters, while freezing the original model parameters.&lt;/li&gt;
  &lt;li&gt;It allows for efficient fine-tuning of language models, reducing the memory footprint and computational requirements.&lt;/li&gt;
  &lt;li&gt;PEFT enables the injection of niche expertise into a foundation model without catastrophic forgetting, preserving the original model’s performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LoRA (Low Rank Adapters):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LoRA is a technique that introduces low-rank adapters for fine-tuning large language models, allowing for efficient backpropagation of gradients through a frozen, quantized pretrained model.&lt;/li&gt;
  &lt;li&gt;It involves configuring parameters such as attention dimension, alpha parameter for scaling, dropout probability, and task type for the language model.&lt;/li&gt;
  &lt;li&gt;LoRA aims to reduce memory usage and computational requirements during fine-tuning, making it possible to train large models on a single GPU while preserving performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These techniques, when combined, enable the efficient fine-tuning of large language models, making the process more accessible and resource-efficient for researchers and practitioners.&lt;/p&gt;

&lt;p&gt;For more information on LoRA refer to: &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot;&gt;https://arxiv.org/abs/2305.14314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For a code example refer to: &lt;a href=&quot;https://github.com/jamesdhope/LLM-fine-tuning/blob/main/tuning.py&quot;&gt;https://github.com/jamesdhope/LLM-fine-tuning/blob/main/tuning.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Code Attribution: Maxime Labonne&lt;/p&gt;</content><author><name>James Hope</name></author><category term="generativeAI" /><category term="conversationalAI" /><category term="llms" /><category term="generativeAI" /><category term="architecture" /><category term="fine tuning" /><category term="watsonx" /><summary type="html">Fine-tuning of a large language model (LLM) can be peformed using QLoRA (Quantized Low Rank Adapters) and PEFT (Parameter-Efficient Fine-Tuning) techniques.</summary></entry><entry><title type="html">Extending a conversational assistant with RAG for conversational search across multiple user and user-group embeddings</title><link href="/generativeai/conversationalai/virtual-assistant-conversational-search/" rel="alternate" type="text/html" title="Extending a conversational assistant with RAG for conversational search across multiple user and user-group embeddings" /><published>2023-11-04T00:00:00+00:00</published><updated>2023-11-04T00:00:00+00:00</updated><id>/generativeai/conversationalai/virtual-assistant-conversational-search</id><content type="html" xml:base="/generativeai/conversationalai/virtual-assistant-conversational-search/">&lt;p&gt;Retrieval Augmented Generation (RAG), which utilises a LLM, makes it relatively straightfoward to surface information through a conversational assistant. This is potentially transformative for HR &amp;amp; talent management and customer care use cases where information contained in policies, guidelines, handbooks and other unstructured natural language formats can be made more accessible and conveniently queried through an assistant’s natural language interface. Here I share an architecture that extends a conversational assistant with RAG, routing searches to collections mapped to a user and intent.&lt;/p&gt;

&lt;p&gt;The key concept are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a data pipeline is run that chunks and embeds policies, guidelines, handbooks and other &lt;em&gt;source information&lt;/em&gt; as collections in the vectorstore. Collections may be specific to a user, group of users or all users&lt;/li&gt;
  &lt;li&gt;a map is created for the RAG router to associate &lt;em&gt;user context&lt;/em&gt; and &lt;em&gt;intent&lt;/em&gt; with one or more collections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When RAG is invoked from the assistant:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the assistant calls the RAG router passing the &lt;em&gt;user context&lt;/em&gt; and &lt;em&gt;intent&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;the RAG router maps the &lt;em&gt;user context&lt;/em&gt; and &lt;em&gt;intent&lt;/em&gt; to one or more (vectorised and embedded) collections&lt;/li&gt;
  &lt;li&gt;the RAG router (1) retrieves semantically similar chunks to the user query from the mapped collections (2) injects results into the prompt (3) generates a response to the user query using the prompt (i.e. executes RAG or some variation of)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/assistant-rag.png&quot; alt=&quot;GitHub Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Variations of and extensions to this architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;placing RAG execution logic within the assistant for higher coupling, lower cohesion trade-off of components executing RAG logic&lt;/li&gt;
  &lt;li&gt;extending data pipelines to read and embed &lt;em&gt;structured&lt;/em&gt; data (e.g. via the watsonx.ai lakehouse prestoDB engine)&lt;/li&gt;
  &lt;li&gt;introducing a pipeline orchestrator such as Watson Pipelines to maintain embeddings according to data validity requirements&lt;/li&gt;
  &lt;li&gt;variations on RAG such a post retrieval ranking&lt;/li&gt;
  &lt;li&gt;variations on chunking such as overlap&lt;/li&gt;
  &lt;li&gt;indexing to optimise search, see https://milvus.io/docs/build_index.md&lt;/li&gt;
  &lt;li&gt;variations on searching, see: https://milvus.io/docs/search.md&lt;/li&gt;
&lt;/ul&gt;</content><author><name>James Hope</name></author><category term="generativeAI" /><category term="conversationalAI" /><category term="llms" /><category term="generativeAI" /><category term="architecture" /><category term="conversationalAI" /><category term="watsonx" /><category term="rag" /><summary type="html">Retrieval Augmented Generation (RAG), which utilises a LLM, makes it relatively straightfoward to surface information through a conversational assistant. This is potentially transformative for HR &amp;amp; talent management and customer care use cases where information contained in policies, guidelines, handbooks and other unstructured natural language formats can be made more accessible and conveniently queried through an assistant’s natural language interface. Here I share an architecture that extends a conversational assistant with RAG, routing searches to collections mapped to a user and intent.</summary></entry><entry><title type="html">An LLM assisted approach to automating testing of a virtual assistant</title><link href="/generativeai/automation/llm-assisted-virtual-assistant-automated-testing/" rel="alternate" type="text/html" title="An LLM assisted approach to automating testing of a virtual assistant" /><published>2023-11-01T00:00:00+00:00</published><updated>2023-11-01T00:00:00+00:00</updated><id>/generativeai/automation/llm-assisted-virtual-assistant-automated-testing</id><content type="html" xml:base="/generativeai/automation/llm-assisted-virtual-assistant-automated-testing/">&lt;p&gt;Large Language Models (LLMs) can be used to automate testing of virtual assistants. One approach is to use the LLM to generate the queries and responses of the human user to automate the test of a journey, end to end. Here I share a conceptual data pipeline view of such a system. The key ideas are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;prompts&lt;/em&gt; are created to generate responses that fulfil the different types of interactions (sometimes called nodes) in the virtual assistant journeys&lt;/li&gt;
  &lt;li&gt;data on &lt;em&gt;intents&lt;/em&gt; and &lt;em&gt;personas&lt;/em&gt; is fetched from file and injected into the prompt and sent to the LLM to generate initial and subsequent queries / responses&lt;/li&gt;
  &lt;li&gt;a &lt;em&gt;code function&lt;/em&gt; is written that orchestrates the interaction between the LLM and the virtual assistant by using prompts and formatting payloads for each type of node&lt;/li&gt;
  &lt;li&gt;a &lt;em&gt;global code function&lt;/em&gt; iterates over intents (by generating different initial queries) and personas for each journey&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/assistant-test-pipeline-view.png&quot; alt=&quot;GitHub Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here I am using the LLM in an &lt;em&gt;assisted&lt;/em&gt; role, where a &lt;em&gt;code function prescribes logic&lt;/em&gt; that maps the prompt to a particular node. However, by tuning the LLM on interactions for each node type, the LLM may be used to drive the automation without the need for a code function to orchestrate individual interactions along the journey.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="generativeAI" /><category term="automation" /><category term="llms" /><category term="generativeAI" /><category term="architecture" /><summary type="html">Large Language Models (LLMs) can be used to automate testing of virtual assistants. One approach is to use the LLM to generate the queries and responses of the human user to automate the test of a journey, end to end. Here I share a conceptual data pipeline view of such a system. The key ideas are:</summary></entry><entry><title type="html">Graph-Driven, LLM-Assisted Virtual Assistant Architecture</title><link href="/generativeai/architecture/graph-driven-llm-assistant-virtual-assistant/" rel="alternate" type="text/html" title="Graph-Driven, LLM-Assisted Virtual Assistant Architecture" /><published>2023-10-02T00:00:00+01:00</published><updated>2023-10-02T00:00:00+01:00</updated><id>/generativeai/architecture/graph-driven-llm-assistant-virtual-assistant</id><content type="html" xml:base="/generativeai/architecture/graph-driven-llm-assistant-virtual-assistant/">&lt;p&gt;View the post here: &lt;a href=&quot;https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040&quot;&gt;https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040&lt;/a&gt;.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="generativeAI" /><category term="architecture" /><category term="data science" /><category term="generativeAI" /><category term="llms" /><category term="virtual assistants" /><category term="graph" /><category term="watsonx" /><category term="ibm" /><category term="architecture" /><summary type="html">View the post here: https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040.</summary></entry><entry><title type="html">Gas System of the Future, Digital Twin</title><link href="/digital%20twins/architecture/gas-system-of-the-future/" rel="alternate" type="text/html" title="Gas System of the Future, Digital Twin" /><published>2022-12-02T00:00:00+00:00</published><updated>2022-12-02T00:00:00+00:00</updated><id>/digital%20twins/architecture/gas-system-of-the-future</id><content type="html" xml:base="/digital%20twins/architecture/gas-system-of-the-future/">&lt;p&gt;This article was published on Medium. Please click &lt;a target=&quot;_new&quot; href=&quot;https://jamesdhope.medium.com/gas-system-of-the-future-digital-twin-9e1622024462&quot;&gt;here&lt;/a&gt; to access the article.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="digital twins" /><category term="architecture" /><category term="architecture" /><category term="energy &amp; utilities" /><category term="digital twins" /><category term="future energy system" /><category term="energy vector integration" /><category term="ibm" /><category term="IBM App Connect Enterprise" /><category term="IBM API Connect" /><category term="Red Hat OpenShift" /><category term="predictive modelling" /><summary type="html">This article was published on Medium. Please click here to access the article.</summary></entry><entry><title type="html">Injecting Config as Environment Variables from Hasicorp’s Consul &amp;amp; Vault</title><link href="/kubernetes/vault-template-2/" rel="alternate" type="text/html" title="Injecting Config as Environment Variables from Hasicorp’s Consul &amp;amp; Vault" /><published>2021-11-19T00:00:00+00:00</published><updated>2021-11-19T00:00:00+00:00</updated><id>/kubernetes/vault-template-2</id><content type="html" xml:base="/kubernetes/vault-template-2/">&lt;p&gt;Continuing with the theme of Kubernetes, I have recently built out a solution to inject environment variables into containerised applications from Hasicorp Consult and Vault Key Value (KV) engine, which might be considered as a first step in realising Hashicorp’s Service Mesh. Installing both Consul and Vault via helm with the KV Engine is fairly straightforward. Supplying these KV’s as environment variables to the containerised applications in Kubernetes, however, requires a bit more thought. Two different approaches are required to lift in values from Consul and Vault which makes things even more interesting. The approach I took was to write the KV’s to file before they are exposed as ENVs in the container, which is less than ideal. As a side note, it might be cleaner and simpler to manage config at the application layer by calling Consul and Vault’s HTTP API. That is another approach which I’m not going to talk about here.&lt;/p&gt;

&lt;p&gt;For the purposes of this explanation I’m supplying environment variables to an application called Hasura graphQL API. Config is held in Consul and Vault in their respective KV Engines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/containers.jpg&quot; alt=&quot;GitHub Logo&quot; /&gt;
Source: https://www.pexels.com/&lt;/p&gt;

&lt;h2 id=&quot;fetching-kvs-from-consul-using-envconsul&quot;&gt;Fetching KV’s from Consul using envconsul&lt;/h2&gt;

&lt;p&gt;Hasicorp provide a application called envconsul to fetch KV’s from consul. I used that to write the KV’s to a mounted volume ready to run as a bash script to expose them in the main application container. Note the use of the sed command to substitue the export statement. This achieves a file with lines that read &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export key value&lt;/code&gt; which can be run as a bash script to expose those KV’s as ENVs in the main application container.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;initContainers:
- name: envconsul
        image: hashicorp/envconsul:alpine
        ## 
        command: [&apos;sh&apos;,&apos;-c&apos;,&apos;envconsul -consul-addr=consul-consul-server.consul.svc.cluster.local:8500 -pristine -prefix staging/hasura env | sed &quot;s/.*/export &amp;amp;/&quot; &amp;gt; /consul/config/staging-hasura&apos;]
- mountPath: /consul/config/
        name: consul
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;fetching-kvs-from-vault-using-vault-annotations&quot;&gt;Fetching KV’s from Vault using Vault Annotations&lt;/h2&gt;

&lt;p&gt;To fetch secrets from Vault I used the annotations shown in the snippet below which are patched to the deployment for Hasura. Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vault.hashicorp.com/agent-inject: &quot;true&quot;&lt;/code&gt; label in the code snippet below which activates the Envoy sidecar although there is more going on here including an initContainer for Vault to initiate the mTLS mechanism so I’d recommend reading the docs. See https://www.vaultproject.io/docs/platform/k8s/injector/annotations.&lt;/p&gt;

&lt;p&gt;The label I wanted to give special mention to is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vault.hashicorp.com/agent-inject-template-staging-hasura:&lt;/code&gt; which utilises a GO template. The template shown in the code snippet below writes the key value pairs in vault (prefixed with the word export) to file, ready to run as a bash script. This writes the same file output as the command for envconsul above.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;`kubectl patch deployment hasura --patch &quot;$(cat staging-vault-patch.yaml)&quot; -n hasura`
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spec:
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: &quot;true&quot;
        vault.hashicorp.com/agent-inject-status: &quot;update&quot;
        vault.hashicorp.com/agent-inject-secret-staging-hasura: &quot;staging/hasura&quot;
        vault.hashicorp.com/agent-inject-template-staging-hasura: |
          
              export =
            
        vault.hashicorp.com/role: &quot;vault-hasura-service-account&quot;
        vault.hashicorp.com/log-level: &quot;debug&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that Vault also requires you to set up a Kubernetes Service Account &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serviceAccountName: vault-service&lt;/code&gt;. Refer to the Hashicorp docs for more information on this.&lt;/p&gt;

&lt;h2 id=&quot;exposing-the-kvs-into-the-main-application-container&quot;&gt;Exposing the KV’s into the main application container&lt;/h2&gt;

&lt;p&gt;Hasura deployment spec can be updated with a command statement: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;command: [&apos;sh&apos;, &apos;-c&apos;, &apos;. vault/secrets/staging-hasura &amp;amp;&amp;amp; . consul/config/staging-hasura &amp;amp;&amp;amp; graphql-engine serve&apos;]&lt;/code&gt;. Using sh with the dot command to execute the bash scripts exposing the KV’s as environment variables in the main application container, and then launching into the application itself. If you take this approach, be aware that the Kubernetes command statement will take precedence over any entrypoint command used in the dockerfile. If you are utilising a third party image that utilises an entrypoint statement you may need to create a custom dockerfile to make this approach work.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="kubernetes" /><category term="architecture" /><category term="hashicorp consul" /><category term="neo4j" /><category term="containerisation" /><category term="hashicorp vault" /><summary type="html">Continuing with the theme of Kubernetes, I have recently built out a solution to inject environment variables into containerised applications from Hasicorp Consult and Vault Key Value (KV) engine, which might be considered as a first step in realising Hashicorp’s Service Mesh. Installing both Consul and Vault via helm with the KV Engine is fairly straightforward. Supplying these KV’s as environment variables to the containerised applications in Kubernetes, however, requires a bit more thought. Two different approaches are required to lift in values from Consul and Vault which makes things even more interesting. The approach I took was to write the KV’s to file before they are exposed as ENVs in the container, which is less than ideal. As a side note, it might be cleaner and simpler to manage config at the application layer by calling Consul and Vault’s HTTP API. That is another approach which I’m not going to talk about here.</summary></entry><entry><title type="html">Backup and Restore Neo4j in a Casual Cluster</title><link href="/kubernetes/architecture/neo4j-backup-restore/" rel="alternate" type="text/html" title="Backup and Restore Neo4j in a Casual Cluster" /><published>2021-11-11T00:00:00+00:00</published><updated>2021-11-11T00:00:00+00:00</updated><id>/kubernetes/architecture/neo4j-backup-restore</id><content type="html" xml:base="/kubernetes/architecture/neo4j-backup-restore/">&lt;p&gt;If you’re managing a data engine inside a kubernetes cluster then implementing a backup and restore process can be challenging. A few months ago I developed a solution architecture deploying Neo4j into Kubernetes as a casual cluster. There’s a Medium post by Neo4j’s David Allen to explain what that configuration looks like &lt;a target=&quot;_new&quot; href=&quot;https://medium.com/neo4j/querying-neo4j-clusters-7d6fde75b5b4&quot;&gt;here&lt;/a&gt;. Unfortunately Neo4j doesn’t officially support a casual cluster deployment, but there are community maintained helm charts endorsed by Neo4j that make this achieveable. For this solution I needed a simple backup and restore (nothing more) which is what I wanted to focus on here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/containers.jpg&quot; alt=&quot;GitHub Logo&quot; /&gt;
Source: https://www.pexels.com/&lt;/p&gt;

&lt;h2 id=&quot;technology-native-versus-snapshots&quot;&gt;Technology-native versus Snapshots&lt;/h2&gt;

&lt;p&gt;The approach of snapshotting persistent volumes for a distributed data engine as a means to backup data can and does lead to situations where a subsequent restore will fail because of an inconsistent state. In this situation a transactional database should run the transactions from the write-ahead logs but I ran into this exact issue when testing this approach with Velero and Neo4j and was unable to complete the restore. Postgres and timescaledb also failed to restore using this approach. Implementing a primary backup and restore mechanism using the officially supported, native database tools (for neo4j the neo4j-backup utility) is my recommended approach.&lt;/p&gt;

&lt;p&gt;For Neo4j, the community helm chart includes a child helm chart for backing up to AWS, GCP or Azure. The helm chart utilises the neo4j-admin backup image provided by Neo4j which runs as a sidecar to neo4j. That approach works well if you want to backup to these providers but if you are backing up to an alternative provider like Digital Ocean it might make more sense to start over and work towards an implementation that is customised to your environment, has less bloat and is easier to maintain. Here’s how I did it.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-cronjob-to-backup&quot;&gt;Kubernetes CronJob to Backup&lt;/h2&gt;

&lt;p&gt;For backup, I created a Kubernetes CronJob. The backup happens in two steps.&lt;/p&gt;

&lt;h3 id=&quot;step-1&quot;&gt;Step 1:&lt;/h3&gt;
&lt;p&gt;The neo4j-backup utility is run as an initialisation container. This produces an online backup which is written to a mounted volume. There is no downtime involved here but be aware that this will have performance implications on your running database. The schedule is set to meet the recovery point objective.&lt;/p&gt;

&lt;h3 id=&quot;step-2&quot;&gt;Step 2:&lt;/h3&gt;
&lt;p&gt;A custom container runs which copies the backup (from the mounted volume) to Digital Ocean S3 using s3cmd. The reason for the custom container here is that at the time of writing there was no easy way to set the s3cmd configuration values at runtime using the CLI so this is configured at the application layer and baked into the image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: backupdir-neo4j
  labels:
    app: neo4j-backup
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: neo4j-backup
  namespace: neo4j
spec:
  schedule: &quot;0 * * * *&quot;
  jobTemplate:
    spec:
      template:
        spec:
          volumes:
            - name: backupdir-neo4j
              persistentVolumeClaim:
                claimName: backupdir-neo4j
          initContainers:
            - name: neo4j-backup
              image: neo4j:4.2.8-enterprise
              env:
                - name: NEO4J_ACCEPT_LICENSE_AGREEMENT
                  value: &quot;yes&quot;
              volumeMounts:
              - name: backupdir-neo4j
                mountPath: /tmp
              command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
              args:
                - echo cleaning /tmp from PV;
                  rm -rf /tmp/*;
                  bin/neo4j-admin backup --backup-dir /tmp --database neo4j --from neo-neo4j.neo4j.svc.cluster.local:6362 --verbose;
                  echo backup completed
          containers:
            - name: copy-to-spaces
              image: registry.gitlab.com/custom-neo-backup-tool:latest
              imagePullPolicy: Always
              command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
              args:
                - yum install python36 -y;
                  pip3 install s3cmd;
                  cp /usr/app/s3cfg /root/.s3cfg;
                  s3cmd --config=/usr/app/s3cfg;
                  s3cmd put /tmp s3://path/to/backup/`date +%d%b%Y-%H:%M:%S`/ --recursive;
              volumeMounts:
              - name: backupdir-neo4j
                mountPath: /tmp
          imagePullSecrets: 
          - name: gitlab-registry-credentials  
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;restore&quot;&gt;Restore&lt;/h2&gt;

&lt;p&gt;The restore process happens in two parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A initContainer runs in the helm chart to copy the data from S3.&lt;/li&gt;
  &lt;li&gt;A command is run inside the POD to restore the backup&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-1-1&quot;&gt;Step 1:&lt;/h3&gt;
&lt;p&gt;The initContainer is a custom built image with the S3cmd config that copies the backup specified into the plugins mount. Note that the path to the backup in the s3cmd get command needs to be specified.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: custom-neo-recovery-tool
    image: registry.gitlab.com/custom-neo-backup-tool
    imagePullPolicy: Always
    volumeMounts:
    - name: plugins
      mountPath: /plugins
    command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
    args:
      - yum install python36 -y;
        pip3 install s3cmd;
        cp /usr/app/s3cfg /root/.s3cfg;
        s3cmd --config=/usr/app/s3cfg;
        s3cmd get --recursive --force s3://path/to/backup/timestamp/ /plugins;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-1&quot;&gt;Step 2:&lt;/h3&gt;
&lt;p&gt;Once the neo core has started to perform the recovery:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. In CYPHER-SHELL OR NEO4j BROWSER run: STOP DATABASE {name}
2. On Pod run: bin/neo4j-admin restore --from /plugins/tmp/neo4j --database neo4j --force;
3. In CYPHER-SHELL or NEO4J BROWSER run: START DATABASE {name}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>James Hope</name></author><category term="kubernetes" /><category term="architecture" /><category term="causual cluster" /><category term="neo4j" /><category term="backup &amp; restore" /><category term="kubernetes" /><category term="architecture" /><summary type="html">If you’re managing a data engine inside a kubernetes cluster then implementing a backup and restore process can be challenging. A few months ago I developed a solution architecture deploying Neo4j into Kubernetes as a casual cluster. There’s a Medium post by Neo4j’s David Allen to explain what that configuration looks like here. Unfortunately Neo4j doesn’t officially support a casual cluster deployment, but there are community maintained helm charts endorsed by Neo4j that make this achieveable. For this solution I needed a simple backup and restore (nothing more) which is what I wanted to focus on here.</summary></entry><entry><title type="html">Top 10 architectural highlights for Digital Ocean Kubernetes</title><link href="/kubernetes/architecture/kubernetes-digital-ocean/" rel="alternate" type="text/html" title="Top 10 architectural highlights for Digital Ocean Kubernetes" /><published>2021-10-27T00:00:00+01:00</published><updated>2021-10-27T00:00:00+01:00</updated><id>/kubernetes/architecture/kubernetes-digital-ocean</id><content type="html" xml:base="/kubernetes/architecture/kubernetes-digital-ocean/">&lt;p&gt;Recently I’ve been developing a solution architecture for a boostrapped startup in Digital Ocean’s Kubernetes. Developing an understanding of the context, discovering the domain and taking initial ideas through critical design thinking has been key to a foundational architecture that should serve this product well throughout its lifecycle. As envisioning has happened, the solution and its architecture has evolved to enable numerous product iterations, building out only what has been necessary at each stage. The domain driven approach to development led to a server based query gateway and so what unfolded was containerised microservcies architecture orchestrated by Kubernetes. Here are my top 10 highlights from building in Digital Ocean Kubernetes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/containers.jpg&quot; alt=&quot;GitHub Logo&quot; /&gt;
Source: https://www.pexels.com/&lt;/p&gt;

&lt;h3 id=&quot;10-utilise-external-infrastructure-for-completeness-when-necessary&quot;&gt;10. Utilise external infrastructure for completeness when necessary&lt;/h3&gt;

&lt;p&gt;The absence of key architectural components to deploy in front of the cluster is a limitation to be aware with Digital Ocean. If you are building for production route traffic via services you can route traffic through CloudFlare, for example, for a layer 7 firewall, OWASP compliance and global server load balancing.&lt;/p&gt;

&lt;h3 id=&quot;9-work-from-the-application-resource-requirements-to-determine-the-minimum-viable-infrastructure&quot;&gt;9. Work from the application resource requirements to determine the minimum viable infrastructure&lt;/h3&gt;

&lt;p&gt;If like most you have machine specifications to provision into your cluster (AWS Fargate the notable exception) then it makes sense to understand what resources your applications need and then work down the stack. One approach is to group applications into logical planes - Control, User, Data plane - to determine what resources are needed in each plane. Also look to vendors and application specifications for guidance on resource and scaling requirements.&lt;/p&gt;

&lt;h3 id=&quot;8-consider-the-velocity-of-scaling-in-the-vertical-and-horizonal-directions-and-the-impact-on-services&quot;&gt;8. Consider the velocity of scaling in the vertical and horizonal directions and the impact on services&lt;/h3&gt;

&lt;p&gt;For horizontal scaling there is speed to think about: kubernetes will replicate pods across nodes in a matter of seconds but if new nodes are required to achieve that replication that it can take minutes. The trade-off here is between performance efficiency and cost optimisation. Set the autoscaling thresholds so there is enough spare capacity in the pods to allow time for the autoscaling to happen or provision larger nodes that enable sideways replication of pods on that same node. Develop event driven microservices with messaging queues to add resiliency. In production, use metrics to determine and refine the right vertical and horizontal thresholds.&lt;/p&gt;

&lt;h3 id=&quot;7-strive-for-the-rule-of-three-for-high-availability&quot;&gt;7. Strive for the ‘rule of three’ for high availability&lt;/h3&gt;

&lt;p&gt;For a production and staging deployment I like to follow the rule of three. Three nodes in three availability zones with three pod replicas in each zone. For stateful applications being deployed into a cluster configuration it is recommended to have three cores or leader-eligible instances to avoid split brain.&lt;/p&gt;

&lt;h3 id=&quot;6-build-with-open-source-multi-vendor-or-community-developed-applications-for-portability&quot;&gt;6. Build with open-source, multi-vendor or community-developed applications for portability&lt;/h3&gt;

&lt;p&gt;If you can build with open-source, multi-vendor and community-developed applications you can avoid vendor lock-in and keep the door open to other clouds as needs evolve over time. For example, in my case, building with Hasura GraphQL rather than AWS AppSync as a graph QL gateway, and Neo4J rather than AWS Neptune as a graph data engine.&lt;/p&gt;

&lt;h3 id=&quot;5-avoid-constraints-and-design-with-soft-intent-for-scheduling-flexibility&quot;&gt;5. Avoid constraints and design with soft intent for scheduling flexibility&lt;/h3&gt;

&lt;p&gt;Imposing hard constraints such as anti-affinity rules, taints and tolerations could result in a pod being unschedulable. Unless you need to import hard constraints use soft requirements such as topology keys to describe scheduling intent and best effort across nodes.&lt;/p&gt;

&lt;h3 id=&quot;4-strive-to-understand-the-limitations-of-the-network-backbone&quot;&gt;4. Strive to understand the limitations of the network backbone&lt;/h3&gt;

&lt;p&gt;Be aware of the limitations of the backbone. Public clouds vary significantly in their network speed. All the autoscaling in the world won’t help if the bottleneck is the backbone.&lt;/p&gt;

&lt;h3 id=&quot;3-use-a-service-mesh-for-resiliency&quot;&gt;3. Use a service mesh for resiliency&lt;/h3&gt;

&lt;p&gt;Since Digital Ocean doesn’t provision Kubernetes with the Kubernetes Networking Plugin a single control plane is limited to orchestrating pods across a single availability zone. That doesn’t need to be an impediment to high availibility though, since with a service mesh (e.g., Traefik, Itsio or Consul) high availibilility can be achieved through the service mesh gateways that enable applications to connect to services that route to pods in clusters in other regional data centers. Relying on the service mesh for service availability could be a good trade-off if the only way to achieve control plane replication and orchestration across availablility zones means looking to more mature and costly platforms. Bear in mind that in a mesh, as applications run at the edge, regional data centers start behaving a bit like secondary availability zones.&lt;/p&gt;

&lt;h3 id=&quot;2-use-managed-services-to-abstract-devops-from-infrastructure-details&quot;&gt;2. Use managed services to abstract devOps from infrastructure details&lt;/h3&gt;

&lt;p&gt;Understanding the ops environment that the solution is being deployed into is key for a successful operation. If the ops environment is not assessed to be ready to operate the applications being proposed, moving them into a managed service can be a good option. This is where the marketplace shines because self-managing data engines (especially in clustered or fully distributed configurations) would most certainly warrant a dedicated team of site reliability engineers trained on the native technology, its disaster recovery procedures amongst other things. As a managed service however, availability, scaling, and disaster recovery (including point-in-time recovery to the nearest second) are trivial to configure. My view is that money is well spent here to abstract debt-laden DevOps from application infrastructure and to enable the focus firmly on the product and hypothesis-driven development.&lt;/p&gt;

&lt;h3 id=&quot;1-use-a-favourable-pricing-model-to-get-into-production&quot;&gt;1. Use a favourable pricing model to get into production&lt;/h3&gt;

&lt;p&gt;For whatever Digital Ocean might lack in edge services and availability zones it makes up for in infrastructure costs. VM pricing per hour is competitive even against the usage discounting applied by the major public cloud providers to the extent that it could extend the of life of a startup and its funding significantly. The virtual private cloud is provided at no cost and data egress is not chargable, which depending on what you are building, could present a significant cost saving (though be way of the limits of the network).  Building with open-source, multi-vendor and community-developed applications on an open platform means porting to another cloud is an option later on when services at the edge and secondary availability zones is probably going to make more sense anyway.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="kubernetes" /><category term="architecture" /><category term="architecture" /><category term="kubernetes" /><category term="open source" /><summary type="html">Recently I’ve been developing a solution architecture for a boostrapped startup in Digital Ocean’s Kubernetes. Developing an understanding of the context, discovering the domain and taking initial ideas through critical design thinking has been key to a foundational architecture that should serve this product well throughout its lifecycle. As envisioning has happened, the solution and its architecture has evolved to enable numerous product iterations, building out only what has been necessary at each stage. The domain driven approach to development led to a server based query gateway and so what unfolded was containerised microservcies architecture orchestrated by Kubernetes. Here are my top 10 highlights from building in Digital Ocean Kubernetes:</summary></entry><entry><title type="html">Can your People Analytics do this?</title><link href="/knowledge%20graphs/can-your-people-analytics-do-this/" rel="alternate" type="text/html" title="Can your People Analytics do this?" /><published>2019-08-02T00:00:00+01:00</published><updated>2019-08-02T00:00:00+01:00</updated><id>/knowledge%20graphs/can-your-people-analytics-do-this</id><content type="html" xml:base="/knowledge%20graphs/can-your-people-analytics-do-this/">&lt;p&gt;This article was published in the journal Towards Data Science. Please click &lt;a target=&quot;_new&quot; href=&quot;https://towardsdatascience.com/can-your-people-analytics-do-this-739afa714f1a&quot;&gt;here&lt;/a&gt; to access the article.&lt;/p&gt;</content><author><name>James Hope</name></author><category term="knowledge graphs" /><category term="people analytics" /><category term="graph" /><category term="neo4j" /><summary type="html">This article was published in the journal Towards Data Science. Please click here to access the article.</summary></entry></feed>